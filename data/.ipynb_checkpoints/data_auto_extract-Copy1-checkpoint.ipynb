{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8866c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import schedule\n",
    "import time\n",
    "import threading\n",
    "import glob\n",
    "import os.path\n",
    "from pycoingecko import CoinGeckoAPI\n",
    "\n",
    "volt01_uu = 'volt01_uu.csv'\n",
    "volt01_ts = 'volt01_ts.csv'\n",
    "volt02_uu = 'volt02_uu.csv'\n",
    "\n",
    "\n",
    "def run_query(query):  # A simple function to use requests.post to make the API call.\n",
    "    headers = {'X-API-KEY': 'BQYCaXaMZlqZrPCSQVsiJrKtxKRVcSe4'}\n",
    "    request = requests.post('https://graphql.bitquery.io/', json={'query': query}, headers=headers)\n",
    "    if request.status_code == 200:\n",
    "        return request.json()\n",
    "    else:\n",
    "        raise Exception('Query failed and return code is {}.{}'.format(request.status_code, query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad77ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_volt_data(csv_name):\n",
    "    # extract data from bitquery only at 30 min intervals\n",
    "    # threading.Timer(120.0, extract_volt_data).start()\n",
    "    \n",
    "    # search for existing asset.csv in data folder\n",
    "    # REMEMBER TO CHANGE PATH DIRECTORY!\n",
    "    csv_files = glob.glob(os.path.join('C:/Users/JOTHAM/Desktop/DeFi/Coding/Friktion Project/dashapp/data', csv_name))\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(csv_name + \" missing\")\n",
    "        query_date = '\"2021-12-16\", \"2022-01-30\"'\n",
    "        if csv_name == 'volt01_uu.csv':\n",
    "            extract_volt01_uu(query_date, csv_files)\n",
    "            action = \"Volt 01 Unique Users\"\n",
    "        elif csv_name == 'volt01_ts.csv':\n",
    "            extract_volt01_ts(query_date, csv_files)\n",
    "            action = \"Volt 01 Average Tx Size\"\n",
    "        elif csv_name == 'volt02_uu.csv':    \n",
    "            extract_volt02_uu(query_date, csv_files)\n",
    "            action = \"Volt 02 Unique Users\"\n",
    "        \n",
    "        print(datetime.utcnow().strftime('%Y-%m-%d , %H:%M:%S') + \" - Running query of \" + action + \"...\")\n",
    "    \n",
    "    else:\n",
    "        print(csv_name + \" found\")\n",
    "        dataframe = pd.read_csv(csv_name, index_col=0, parse_dates=True)\n",
    "        #---- convert timestamp into string for query date  ----#\n",
    "        # start_date = (pd.to_datetime(dataframe['date'].max()) + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "        # end_date = (pd.to_datetime(dataframe['date'].max()) + timedelta(days=8)).strftime('%Y-%m-%d')\n",
    "        # query_date = '\"' + start_date + '\"' + ',' + '\"' + end_date + '\"'\n",
    "        \n",
    "        dataframe.index = pd.to_datetime(dataframe['date'], format = '%Y-%m-%d')\n",
    "        # get 1 day before current date. This ensures that one full day of volt transactions are \n",
    "        # captured on solscan\n",
    "        utc_date_bef = datetime.utcnow().date() - timedelta(days=1) # solscan is in UTC\n",
    "        # df max date + 1\n",
    "        start_date_query = dataframe.index.max().date() + timedelta(days=1)\n",
    "        # if df max date is less than utc date, \n",
    "        if start_date_query < utc_date_bef:\n",
    "            query_date =  '\"' + start_date_query.strftime('%Y-%m-%d') + '\"' + ',' + '\"' + utc_date_bef.strftime('%Y-%m-%d') + '\"'\n",
    "            if csv_name == 'volt01_uu.csv':\n",
    "                extract_volt01_uu(query_date, csv_files)\n",
    "                action = \"Volt 01 Unique Users\"\n",
    "            elif csv_name == 'volt01_ts.csv':\n",
    "                extract_volt01_ts(query_date, csv_files)\n",
    "                action = \"Volt 01 Average Tx Sizez\"\n",
    "            elif csv_name == 'volt02_uu.csv':    \n",
    "                extract_volt02_uu(query_date, csv_files)\n",
    "                action = \"Volt 02 Unique Users\"\n",
    "            print(datetime.utcnow().strftime('%Y-%m-%d , %H:%M:%S') + \" - Running query of \" + action + \"...\")\n",
    "            \n",
    "        else:\n",
    "            print(datetime.utcnow().strftime('%Y-%m-%d , %H:%M:%S') +  \" - No new data found, running query again in 24 hours.\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89b4e5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_volt01_uu(query_date, csv_files):\n",
    "    # The GraphQL query - Volt01 Call\n",
    "    # 16 Dec 2021 - BTC Volt 01 Call - 3BjcHXvyzMsjmeqE2qFLx45K4XFx3JPiyRnjJiF5MAHt\n",
    "    # 17 Dec 2021 - SOL Volt 01 Call - 4Hnh1UCC6HLzx9NaGKnTVHR2bANcRrhydumdHCnrT3i2\n",
    "    # mSOL Volt 01 Call - 6UA3yn28XecAHLTwoCtjfzy3WcyQj1x13bxnH8urUiKt\n",
    "    # ETH Volt 01 Call - GjnoPUjQiEUYWuKAbMax2cM1Eony8Yutc133wuSun9hS\n",
    "    # FTT Volt 01 Call - 7wDh4VCTPwx41kvbLE6fkFgMEjnqw7NpGJvQtNabCm2B\n",
    "    # SRM Volt 01 Call - 5SLqZSywodLS8ih6U2AAioZrxpgR149hR8SApmCB7r5X\n",
    "    # MNGO Volt 01 Call - 4sTuzTYfcE2NF7zy6Sy8XhVcNLa6JQSLrx3roy97n4sD\n",
    "    # scnSOL Volt 01 Call - 5VmdHqvRMbXivuC34w4Hux9zb1y9moiBEQmXDrTR1kV\n",
    "    # SBR Volt 01 Call - DPMCwE9z9jXaDVDti5aKhdgCWGgsvioz6ZvB9eZjH7UE\n",
    "    # LUNA Volt 01 Call - 95sn4kgeJnnBfRCD8S2quu4HS9Y6vb7JDuXrarnmEjYE\n",
    "    \n",
    "    # Adjust date & account for query\n",
    "    query = \"\"\"\n",
    "    query{\n",
    "    solana(network: solana) {\n",
    "    instructionAccounts(\n",
    "      account: {in: [\n",
    "        \"4Hnh1UCC6HLzx9NaGKnTVHR2bANcRrhydumdHCnrT3i2\",\n",
    "        \"3BjcHXvyzMsjmeqE2qFLx45K4XFx3JPiyRnjJiF5MAHt\",\n",
    "        \"6UA3yn28XecAHLTwoCtjfzy3WcyQj1x13bxnH8urUiKt\",\n",
    "        \"GjnoPUjQiEUYWuKAbMax2cM1Eony8Yutc133wuSun9hS\",\n",
    "        \"7wDh4VCTPwx41kvbLE6fkFgMEjnqw7NpGJvQtNabCm2B\",\n",
    "        \"5SLqZSywodLS8ih6U2AAioZrxpgR149hR8SApmCB7r5X\",\n",
    "        \"4sTuzTYfcE2NF7zy6Sy8XhVcNLa6JQSLrx3roy97n4sD\",\n",
    "        \"5VmdHqvRMbXivuC34w4Hux9zb1y9moiBEQmXDrTR1kV\",\n",
    "        \"DPMCwE9z9jXaDVDti5aKhdgCWGgsvioz6ZvB9eZjH7UE\",\n",
    "        \"95sn4kgeJnnBfRCD8S2quu4HS9Y6vb7JDuXrarnmEjYE\"]}\n",
    "      date: {between: [\"\"\"+ query_date +\"\"\"]}\n",
    "      programId: {is: \"VoLT1mJz1sbnxwq5Fv2SXjdVDgPXrb9tJyC8WpMDkSp\"}\n",
    "      success: {is: true}\n",
    "    ) {\n",
    "      account {\n",
    "        name\n",
    "      }\n",
    "      date {\n",
    "        date\n",
    "      }\n",
    "      transaction {\n",
    "        feePayer\n",
    "      }\n",
    "    }\n",
    "    }\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    result = run_query(query)\n",
    "    # convert GraphQL json to pandas dataframe\n",
    "    df = pd.json_normalize(result['data']['solana']['instructionAccounts'])\n",
    "    # rename to assets name\n",
    "    asset_conditions = [\n",
    "        (df['account.name'] == \"4Hnh1UCC6HLzx9NaGKnTVHR2bANcRrhydumdHCnrT3i2\"),\n",
    "        (df['account.name'] == \"3BjcHXvyzMsjmeqE2qFLx45K4XFx3JPiyRnjJiF5MAHt\"),\n",
    "        (df['account.name'] == \"6UA3yn28XecAHLTwoCtjfzy3WcyQj1x13bxnH8urUiKt\"),\n",
    "        (df['account.name'] == \"GjnoPUjQiEUYWuKAbMax2cM1Eony8Yutc133wuSun9hS\"),\n",
    "        (df['account.name'] == \"7wDh4VCTPwx41kvbLE6fkFgMEjnqw7NpGJvQtNabCm2B\"),\n",
    "        (df['account.name'] == \"5SLqZSywodLS8ih6U2AAioZrxpgR149hR8SApmCB7r5X\"),\n",
    "        (df['account.name'] == \"4sTuzTYfcE2NF7zy6Sy8XhVcNLa6JQSLrx3roy97n4sD\"),\n",
    "        (df['account.name'] == \"5VmdHqvRMbXivuC34w4Hux9zb1y9moiBEQmXDrTR1kV\"),\n",
    "        (df['account.name'] == \"DPMCwE9z9jXaDVDti5aKhdgCWGgsvioz6ZvB9eZjH7UE\"),\n",
    "        (df['account.name'] == \"95sn4kgeJnnBfRCD8S2quu4HS9Y6vb7JDuXrarnmEjYE\"),\n",
    "    ]\n",
    "\n",
    "    asset_Categories = ['SOL', 'BTC', 'mSOL', 'ETH', 'FTT', 'SRM', 'MNGO', 'scnSOL', 'SBR', 'LUNA']\n",
    "    df['account.name'] = np.select(asset_conditions, asset_Categories)\n",
    "    df = df.groupby(['account.name', 'date.date']).size().reset_index(name='count')\n",
    "    # rename columns\n",
    "    df.columns = ['asset', 'date', 'unique_users']\n",
    "    # remove duplicated rows if any\n",
    "    df.drop_duplicates(keep='first',inplace=True)\n",
    "    \n",
    "    if not csv_files:\n",
    "        # save to csv\n",
    "        df.to_csv(\"volt01_uu.csv\")\n",
    "        print(\"Created new volt01_uu.csv file. Inserted data between \" + query_date)\n",
    "    else:\n",
    "        df.to_csv(\"volt01_uu.csv\", mode='a', header=False)\n",
    "        print(\"New data found for date range: \" + query_date)\n",
    "        print(\"Appended new data to volt01_uu.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88064903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_volt01_ts(query_date, csv_files):\n",
    "    \n",
    "    # The GraphQL query - Volt01 Call\n",
    "    # SOL receiving address: Hxtb6APfNtf9m8jJjh7uYp8fCTGr9aeHxBSfiPqCrV6G\n",
    "    # BTC : DA1M8mw7GnPNKU9ReANtHPQyuVzKZtsuuSbCyc2uX2du\n",
    "    # mSOL : 6asST5hurmxJ8uFvh7ZRWkrMfSEzjEAJ4DNR1is3G6eH\n",
    "    # ETH : FThcy5XXvab5u3jbA6NjWKdMNiCSV3oY5AAkvEvpa8wp\n",
    "    # FTT : 7KqHFuUksvNhrWgoacKkqyp2RwfBNdypCYgK9nxD1d6K\n",
    "    # SRM : 2P427N5sYcEXvZAZwqNzjXEHsBMESQoLyjNquTSmGPMb\n",
    "    # MNGO : B3yakZxwomkmnCxRr8ZmQtiWgtxtVBuCREDFDdAvcCVQ\n",
    "    # scnSOL : A5MpyajTy6hdsg3S2em5ukcgY1ZBhxTxEKv8BgHajv1A\n",
    "    # SBR receiving address: BH7Jg3f97FyeGxsPR7FFskvfqGiaLeUnJ9Ksda53Jj8h\n",
    "    # LUNA : 5oV1Yf8q1oQgPYuHjepjmKFuaG2Wng9dzTqbSWhU5W2X\n",
    "    # RAY : A6XsYxGj9wpqUZG81XwgQJ2zJ3efCbuWSQfnkHqUSmdM\n",
    "\n",
    "    # Adjust date & account for query\n",
    "    query = \"\"\"\n",
    "    query{\n",
    "    solana(network: solana) {\n",
    "    transfers(\n",
    "      date: {since: \"2021-12-15\"}\n",
    "      transferType: {is: transfer}\n",
    "      currency: {in: [\"SOL\", \"BTC\", \"mSOL\", \"ETH\", \"FTT\", \"SRM\", \"MNGO\", \"scnSOL\", \"SBR\", \"LUNA\", \"RAY\"]}\n",
    "      any: [{receiverAddress: {in: [\n",
    "        \"Hxtb6APfNtf9m8jJjh7uYp8fCTGr9aeHxBSfiPqCrV6G\",\n",
    "        \"DA1M8mw7GnPNKU9ReANtHPQyuVzKZtsuuSbCyc2uX2du\",\n",
    "        \"6asST5hurmxJ8uFvh7ZRWkrMfSEzjEAJ4DNR1is3G6eH\",\n",
    "        \"FThcy5XXvab5u3jbA6NjWKdMNiCSV3oY5AAkvEvpa8wp\",\n",
    "        \"7KqHFuUksvNhrWgoacKkqyp2RwfBNdypCYgK9nxD1d6K\",\n",
    "        \"2P427N5sYcEXvZAZwqNzjXEHsBMESQoLyjNquTSmGPMb\",\n",
    "        \"B3yakZxwomkmnCxRr8ZmQtiWgtxtVBuCREDFDdAvcCVQ\",\n",
    "        \"A5MpyajTy6hdsg3S2em5ukcgY1ZBhxTxEKv8BgHajv1A\",\n",
    "        \"BH7Jg3f97FyeGxsPR7FFskvfqGiaLeUnJ9Ksda53Jj8h\",\n",
    "        \"5oV1Yf8q1oQgPYuHjepjmKFuaG2Wng9dzTqbSWhU5W2X\",\n",
    "        \"A6XsYxGj9wpqUZG81XwgQJ2zJ3efCbuWSQfnkHqUSmdM\"]}}]\n",
    "    ) {\n",
    "      amount\n",
    "      currency {\n",
    "        symbol\n",
    "      }\n",
    "      date {\n",
    "        date\n",
    "      }\n",
    "      sender {\n",
    "        address\n",
    "      }\n",
    "    }\n",
    "    }\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    result = run_query(query)\n",
    "    # convert GraphQL json to pandas dataframe\n",
    "    df = pd.json_normalize(result['data']['solana']['transfers'])\n",
    "    # sum up amount according to sender addresses by asset grp\n",
    "    agg_functions = {'amount': 'sum'}\n",
    "    df_new = df.groupby(['currency.symbol','sender.address']).agg(agg_functions).reset_index()\n",
    "    # get average size of each asset grp\n",
    "    agg_functions_2 = {'amount': 'mean'}\n",
    "    df_new_2 = df_new.groupby(['currency.symbol']).agg(agg_functions_2).reset_index()\n",
    "    # set assets as index for easy concatenation \n",
    "    df_new_2 = df_new_2.set_index('currency.symbol')\n",
    "\n",
    "    # retrieve asset prices via pycoingecko\n",
    "    \n",
    "    cg = CoinGeckoAPI()\n",
    "\n",
    "    # SOL, BTC, mSOL, ETH, FTT, SRM\", \"MNGO\", \"scnSOL\", \"SBR\", \"LUNA\", \"RAY\"\n",
    "    asset_price = cg.get_price(ids='solana, bitcoin, msol, ethereum, ftx-token, serum, mango-markets, socean-staked-sol, saber, terra-luna, raydium', vs_currencies='usd')\n",
    "    df_price = pd.DataFrame(asset_price)\n",
    "    df_price = df_price.rename(columns={\"solana\": \"SOL\", \"bitcoin\": \"BTC\", \"msol\": \"mSOL\", \n",
    "                             \"ethereum\": \"ETH\", \"ftx-token\": \"FTT\", \"serum\": \"SRM\", \n",
    "                             \"mango-markets\": \"MNGO\", \"socean-staked-sol\": \"scnSOL\", \n",
    "                             \"saber\": \"SBR\", \"terra-luna\": \"LUNA\", \"raydium\": \"RAY\"})\n",
    "    df_price = df_price.T\n",
    "\n",
    "    # Concatenate 2 tables tgt aligning assets\n",
    "    df_new_3 = pd.concat([df_new_2, df_price], axis=1)\n",
    "    # Get average transaction size in USD\n",
    "    df_new_3[\"tx_size\"] = df_new_3[\"amount\"] * df_new_3[\"usd\"]\n",
    "    df_new_3.sort_values(by=['tx_size'], inplace=True)\n",
    "\n",
    "    # save df to csv\n",
    "    if not csv_files:\n",
    "        # save to csv\n",
    "        df_new_3.to_csv(\"volt01_ts.csv\")\n",
    "        print(\"Created new volt01_ts.csv file. Inserted data between \" + query_date)\n",
    "    else:\n",
    "        df_new_3.to_csv(\"volt01_ts.csv\")\n",
    "        #df.to_csv(\"volt01_ts.csv\", mode='a', header=False)\n",
    "        print(\"volt01_ts.csv Updated!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa9a10e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_volt02_uu(query_date, csv_files):\n",
    "    \n",
    "    # The GraphQL query - Volt01 Call\n",
    "    \n",
    "    # For transaction size of Volt 02 put\n",
    "    # LUNA UST receiving address: 5kA7FPiB3t2X5s65dK1AoEu5asDjC5d7f5vaB4iY2yrj\n",
    "    # USDC receiving address: 6Nkc8MEiz3WLz1xthYitmSuy3NGwn7782upRHo2iFmXK\n",
    "    \n",
    "    \n",
    "    # Adjust date & account for query\n",
    "    query = \"\"\"\n",
    "    query{\n",
    "    solana(network: solana) {\n",
    "    instructionAccounts(\n",
    "      account: {in: [\n",
    "        \"4Hnh1UCC6HLzx9NaGKnTVHR2bANcRrhydumdHCnrT3i2\",\n",
    "        \"3BjcHXvyzMsjmeqE2qFLx45K4XFx3JPiyRnjJiF5MAHt\",\n",
    "        \"6UA3yn28XecAHLTwoCtjfzy3WcyQj1x13bxnH8urUiKt\",\n",
    "        \"GjnoPUjQiEUYWuKAbMax2cM1Eony8Yutc133wuSun9hS\",\n",
    "        \"7wDh4VCTPwx41kvbLE6fkFgMEjnqw7NpGJvQtNabCm2B\",\n",
    "        \"5SLqZSywodLS8ih6U2AAioZrxpgR149hR8SApmCB7r5X\",\n",
    "        \"4sTuzTYfcE2NF7zy6Sy8XhVcNLa6JQSLrx3roy97n4sD\",\n",
    "        \"5VmdHqvRMbXivuC34w4Hux9zb1y9moiBEQmXDrTR1kV\",\n",
    "        \"DPMCwE9z9jXaDVDti5aKhdgCWGgsvioz6ZvB9eZjH7UE\",\n",
    "        \"95sn4kgeJnnBfRCD8S2quu4HS9Y6vb7JDuXrarnmEjYE\"]}\n",
    "      date: {between: [\"\"\"+ query_date +\"\"\"]}\n",
    "      programId: {is: \"VoLT1mJz1sbnxwq5Fv2SXjdVDgPXrb9tJyC8WpMDkSp\"}\n",
    "      success: {is: true}\n",
    "    ) {\n",
    "      account {\n",
    "        name\n",
    "      }\n",
    "      date {\n",
    "        date\n",
    "      }\n",
    "      transaction {\n",
    "        feePayer\n",
    "      }\n",
    "    }\n",
    "    }\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    result = run_query(query)\n",
    "    # convert GraphQL json to pandas dataframe\n",
    "    df = pd.json_normalize(result['data']['solana']['instructionAccounts'])\n",
    "    # rename to assets name\n",
    "    asset_conditions = [\n",
    "        (df['account.name'] == \"\"),\n",
    "        (df['account.name'] == \"\"),\n",
    "        (df['account.name'] == \"\"),\n",
    "        (df['account.name'] == \"\"),\n",
    "        (df['account.name'] == \"\"),\n",
    "        (df['account.name'] == \"\"),\n",
    "        (df['account.name'] == \"\"),\n",
    "        (df['account.name'] == \"\"),\n",
    "        (df['account.name'] == \"\"),\n",
    "        (df['account.name'] == \"95sn4kgeJnnBfRCD8S2quu4HS9Y6vb7JDuXrarnmEjYE\"),\n",
    "    ]\n",
    "\n",
    "    asset_Categories = ['SOL', 'BTC', 'mSOL', 'ETH', 'FTT', 'SRM', 'MNGO', 'scnSOL', 'SBR', 'LUNA']\n",
    "    df['account.name'] = np.select(asset_conditions, asset_Categories)\n",
    "    df = df.groupby(['account.name', 'date.date']).size().reset_index(name='count')\n",
    "    # rename columns\n",
    "    df.columns = ['asset', 'date', 'unique_users']\n",
    "    # remove duplicated rows if any\n",
    "    df.drop_duplicates(keep='first',inplace=True)\n",
    "    \n",
    "    if not csv_files:\n",
    "        # save to csv\n",
    "        df.to_csv(\"volt02_uu.csv\")\n",
    "        print(\"Created new volt02_uu.csv file. Inserted data between \" + query_date)\n",
    "    else:\n",
    "        df.to_csv(\"volt02_uu.csv\", mode='a', header=False)\n",
    "        print(\"New data found for date range: \" + query_date)\n",
    "        print(\"Appended new data to \" + csv_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f678b2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_volt02_ts(query_date, csv_files):\n",
    "    \n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64c54f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-30 , 14:40:00 - Running query of Unique Users for Volt01...\n",
      "volt01_uu.csv missing\n",
      "Created new volt01_uu.csv file. Inserted data between \"2021-12-16\", \"2022-01-30\"\n",
      "2022-01-30 , 14:45:00 - Running query of Unique Users for Volt01...\n",
      "volt01_ts.csv missing\n",
      "Created new volt01_ts.csv file. Inserted data between \"2021-12-16\", \"2022-01-30\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-70d4037c8ce8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# is pending to run or not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mschedule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_pending\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# when using .do, argument within function is placed in the following manner\n",
    "schedule.every().day.at(\"22:40\").do(extract_volt_data, volt01_uu)\n",
    "# schedule.every().day.at(\"00:02\").do(extract_volt_data(volt02_uu))\n",
    "schedule.every().day.at(\"22:45\").do(extract_volt_data, volt01_ts)\n",
    "\n",
    "while True:\n",
    "    # Checks whether a scheduled task \n",
    "    # is pending to run or not\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231dd64b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
